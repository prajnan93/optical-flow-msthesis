{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0d2742c-de57-4cf3-bd69-159ce17a8b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Aug 21 12:33:40 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 512.72       Driver Version: 512.72       CUDA Version: 11.6     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0 Off |                  N/A |\n",
      "| N/A   48C    P8     3W /  N/A |      0MiB /  6144MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      8472    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff318a7-e802-4c62-b348-b4e740b74bbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Pytorch Lightning PWCNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b89cda9a-82a8-402e-9dae-faa4125411a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/21/2022 12:33:44 - INFO: Loading faiss with AVX2 support.\n",
      "08/21/2022 12:33:44 - INFO: Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "import ptlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c9ffc57-3805-4d13-8947-f6992f14762f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ptlflow.get_trainable_model_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "955442fb-a702-4dc6-a38e-f48572718a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptlflow.utils.correlation import IterSpatialCorrelationSampler as SpatialCorrelationSampler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2570611c-1ca4-4527-9f52-49dc82524479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc687bdb-bbfe-418f-927e-0efa55f1080d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(in_planes, out_planes, kernel_size=3, stride=1, padding=1, dilation=1):   \n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, \n",
    "                        padding=padding, dilation=dilation, bias=True),\n",
    "            nn.LeakyReLU(0.1))\n",
    "\n",
    "\n",
    "def predict_flow(in_planes):\n",
    "    return nn.Conv2d(in_planes,2,kernel_size=3,stride=1,padding=1,bias=True)\n",
    "\n",
    "\n",
    "def deconv(in_planes, out_planes, kernel_size=4, stride=2, padding=1):\n",
    "    return nn.ConvTranspose2d(in_planes, out_planes, kernel_size, stride, padding, bias=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b497731-7d0a-4837-9294-46f6e8c908c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PWCNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PWCNet, self).__init__()\n",
    "        \n",
    "        self.div_flow = 20.0\n",
    "        \n",
    "        self.md = 4\n",
    "\n",
    "        self.conv1a  = conv(3,   16, kernel_size=3, stride=2)\n",
    "        self.conv1aa = conv(16,  16, kernel_size=3, stride=1)\n",
    "        self.conv1b  = conv(16,  16, kernel_size=3, stride=1)\n",
    "        self.conv2a  = conv(16,  32, kernel_size=3, stride=2)\n",
    "        self.conv2aa = conv(32,  32, kernel_size=3, stride=1)\n",
    "        self.conv2b  = conv(32,  32, kernel_size=3, stride=1)\n",
    "        self.conv3a  = conv(32,  64, kernel_size=3, stride=2)\n",
    "        self.conv3aa = conv(64,  64, kernel_size=3, stride=1)\n",
    "        self.conv3b  = conv(64,  64, kernel_size=3, stride=1)\n",
    "        self.conv4a  = conv(64,  96, kernel_size=3, stride=2)\n",
    "        self.conv4aa = conv(96,  96, kernel_size=3, stride=1)\n",
    "        self.conv4b  = conv(96,  96, kernel_size=3, stride=1)\n",
    "        self.conv5a  = conv(96, 128, kernel_size=3, stride=2)\n",
    "        self.conv5aa = conv(128,128, kernel_size=3, stride=1)\n",
    "        self.conv5b  = conv(128,128, kernel_size=3, stride=1)\n",
    "        self.conv6aa = conv(128,196, kernel_size=3, stride=2)\n",
    "        self.conv6a  = conv(196,196, kernel_size=3, stride=1)\n",
    "        self.conv6b  = conv(196,196, kernel_size=3, stride=1)\n",
    "\n",
    "        self.leakyRELU = nn.LeakyReLU(0.1)\n",
    "\n",
    "        self.corr = SpatialCorrelationSampler(kernel_size=1, patch_size=2*self.md+1, padding=0)\n",
    "        \n",
    "        nd = (2*self.md+1)**2\n",
    "        dd = np.cumsum([128,128,96,64,32])\n",
    "\n",
    "        od = nd\n",
    "        self.conv6_0 = conv(od,      128, kernel_size=3, stride=1)\n",
    "        self.conv6_1 = conv(od+dd[0],128, kernel_size=3, stride=1)\n",
    "        self.conv6_2 = conv(od+dd[1],96,  kernel_size=3, stride=1)\n",
    "        self.conv6_3 = conv(od+dd[2],64,  kernel_size=3, stride=1)\n",
    "        self.conv6_4 = conv(od+dd[3],32,  kernel_size=3, stride=1)        \n",
    "        self.predict_flow6 = predict_flow(od+dd[4])\n",
    "        self.deconv6 = deconv(2, 2, kernel_size=4, stride=2, padding=1) \n",
    "        self.upfeat6 = deconv(od+dd[4], 2, kernel_size=4, stride=2, padding=1) \n",
    "        \n",
    "        od = nd+128+4\n",
    "        self.conv5_0 = conv(od,      128, kernel_size=3, stride=1)\n",
    "        self.conv5_1 = conv(od+dd[0],128, kernel_size=3, stride=1)\n",
    "        self.conv5_2 = conv(od+dd[1],96,  kernel_size=3, stride=1)\n",
    "        self.conv5_3 = conv(od+dd[2],64,  kernel_size=3, stride=1)\n",
    "        self.conv5_4 = conv(od+dd[3],32,  kernel_size=3, stride=1)\n",
    "        self.predict_flow5 = predict_flow(od+dd[4]) \n",
    "        self.deconv5 = deconv(2, 2, kernel_size=4, stride=2, padding=1) \n",
    "        self.upfeat5 = deconv(od+dd[4], 2, kernel_size=4, stride=2, padding=1) \n",
    "        \n",
    "        od = nd+96+4\n",
    "        self.conv4_0 = conv(od,      128, kernel_size=3, stride=1)\n",
    "        self.conv4_1 = conv(od+dd[0],128, kernel_size=3, stride=1)\n",
    "        self.conv4_2 = conv(od+dd[1],96,  kernel_size=3, stride=1)\n",
    "        self.conv4_3 = conv(od+dd[2],64,  kernel_size=3, stride=1)\n",
    "        self.conv4_4 = conv(od+dd[3],32,  kernel_size=3, stride=1)\n",
    "        self.predict_flow4 = predict_flow(od+dd[4]) \n",
    "        self.deconv4 = deconv(2, 2, kernel_size=4, stride=2, padding=1) \n",
    "        self.upfeat4 = deconv(od+dd[4], 2, kernel_size=4, stride=2, padding=1) \n",
    "        \n",
    "        od = nd+64+4\n",
    "        self.conv3_0 = conv(od,      128, kernel_size=3, stride=1)\n",
    "        self.conv3_1 = conv(od+dd[0],128, kernel_size=3, stride=1)\n",
    "        self.conv3_2 = conv(od+dd[1],96,  kernel_size=3, stride=1)\n",
    "        self.conv3_3 = conv(od+dd[2],64,  kernel_size=3, stride=1)\n",
    "        self.conv3_4 = conv(od+dd[3],32,  kernel_size=3, stride=1)\n",
    "        self.predict_flow3 = predict_flow(od+dd[4]) \n",
    "        self.deconv3 = deconv(2, 2, kernel_size=4, stride=2, padding=1) \n",
    "        self.upfeat3 = deconv(od+dd[4], 2, kernel_size=4, stride=2, padding=1) \n",
    "        \n",
    "        od = nd+32+4\n",
    "        self.conv2_0 = conv(od,      128, kernel_size=3, stride=1)\n",
    "        self.conv2_1 = conv(od+dd[0],128, kernel_size=3, stride=1)\n",
    "        self.conv2_2 = conv(od+dd[1],96,  kernel_size=3, stride=1)\n",
    "        self.conv2_3 = conv(od+dd[2],64,  kernel_size=3, stride=1)\n",
    "        self.conv2_4 = conv(od+dd[3],32,  kernel_size=3, stride=1)\n",
    "        self.predict_flow2 = predict_flow(od+dd[4]) \n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "                nn.init.kaiming_normal_(m.weight.data, mode='fan_in')\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "        self.upsample1 = nn.Upsample(scale_factor=4, mode='bilinear', align_corners=True)\n",
    "\n",
    "\n",
    "    def warp(self, x, flo):\n",
    "        \"\"\"\n",
    "        warp an image/tensor (im2) back to im1, according to the optical flow\n",
    "        x: [B, C, H, W] (im2)\n",
    "        flo: [B, 2, H, W] flow\n",
    "        \"\"\"\n",
    "        B, C, H, W = x.size()\n",
    "        # mesh grid \n",
    "        xx = torch.arange(0, W).view(1,-1).repeat(H,1)\n",
    "        yy = torch.arange(0, H).view(-1,1).repeat(1,W)\n",
    "        xx = xx.view(1,1,H,W).repeat(B,1,1,1)\n",
    "        yy = yy.view(1,1,H,W).repeat(B,1,1,1)\n",
    "        grid = torch.cat((xx,yy),1).float()\n",
    "\n",
    "        if x.is_cuda:\n",
    "            grid = grid.to(dtype=x.dtype, device=x.device)\n",
    "        vgrid = grid + flo\n",
    "\n",
    "        # scale grid to [-1,1] \n",
    "        vgrid[:,0,:,:] = 2.0*vgrid[:,0,:,:].clone() / max(W-1,1)-1.0\n",
    "        vgrid[:,1,:,:] = 2.0*vgrid[:,1,:,:].clone() / max(H-1,1)-1.0\n",
    "\n",
    "        vgrid = vgrid.permute(0,2,3,1)        \n",
    "        output = nn.functional.grid_sample(x, vgrid, align_corners=True)\n",
    "        mask = torch.ones(x.size()).to(dtype=x.dtype, device=x.device)\n",
    "        mask = nn.functional.grid_sample(mask, vgrid, align_corners=True)\n",
    "        \n",
    "        mask[mask<0.9999] = 0\n",
    "        mask[mask>0] = 1\n",
    "        \n",
    "        return output*mask\n",
    "\n",
    "\n",
    "    def forward(self, im1, im2):\n",
    "        \n",
    "        H, W = im1.shape[-2:]\n",
    "        # im1 = inputs['images'][:, 0]\n",
    "        # im2 = inputs['images'][:, 1]\n",
    "        \n",
    "        c11 = self.conv1b(self.conv1aa(self.conv1a(im1)))\n",
    "        c21 = self.conv1b(self.conv1aa(self.conv1a(im2)))\n",
    "        c12 = self.conv2b(self.conv2aa(self.conv2a(c11)))\n",
    "        c22 = self.conv2b(self.conv2aa(self.conv2a(c21)))\n",
    "        c13 = self.conv3b(self.conv3aa(self.conv3a(c12)))\n",
    "        c23 = self.conv3b(self.conv3aa(self.conv3a(c22)))\n",
    "        c14 = self.conv4b(self.conv4aa(self.conv4a(c13)))\n",
    "        c24 = self.conv4b(self.conv4aa(self.conv4a(c23)))\n",
    "        c15 = self.conv5b(self.conv5aa(self.conv5a(c14)))\n",
    "        c25 = self.conv5b(self.conv5aa(self.conv5a(c24)))\n",
    "        c16 = self.conv6b(self.conv6a(self.conv6aa(c15)))\n",
    "        c26 = self.conv6b(self.conv6a(self.conv6aa(c25)))\n",
    "\n",
    "\n",
    "        corr6 = self.corr(c16, c26)\n",
    "        corr6 = corr6.view(corr6.shape[0], -1, corr6.shape[3], corr6.shape[4])\n",
    "        corr6 = corr6 / c16.shape[1]\n",
    "        corr6 = self.leakyRELU(corr6)\n",
    "\n",
    "\n",
    "        x = torch.cat((self.conv6_0(corr6), corr6),1)\n",
    "        x = torch.cat((self.conv6_1(x), x),1)\n",
    "        x = torch.cat((self.conv6_2(x), x),1)\n",
    "        x = torch.cat((self.conv6_3(x), x),1)\n",
    "        x = torch.cat((self.conv6_4(x), x),1)\n",
    "        flow6 = self.predict_flow6(x)\n",
    "        up_flow6 = self.deconv6(flow6)\n",
    "        up_feat6 = self.upfeat6(x)\n",
    "\n",
    "        \n",
    "        warp5 = self.warp(c25, up_flow6*0.625)\n",
    "        corr5 = self.corr(c15, warp5)\n",
    "        corr5 = corr5.view(corr5.shape[0], -1, corr5.shape[3], corr5.shape[4])\n",
    "        corr5 = corr5 / c15.shape[1]\n",
    "        corr5 = self.leakyRELU(corr5)\n",
    "        x = torch.cat((corr5, c15, up_flow6, up_feat6), 1)\n",
    "        x = torch.cat((self.conv5_0(x), x),1)\n",
    "        x = torch.cat((self.conv5_1(x), x),1)\n",
    "        x = torch.cat((self.conv5_2(x), x),1)\n",
    "        x = torch.cat((self.conv5_3(x), x),1)\n",
    "        x = torch.cat((self.conv5_4(x), x),1)\n",
    "        flow5 = self.predict_flow5(x)\n",
    "        up_flow5 = self.deconv5(flow5)\n",
    "        up_feat5 = self.upfeat5(x)\n",
    "\n",
    "       \n",
    "        warp4 = self.warp(c24, up_flow5*1.25)\n",
    "        corr4 = self.corr(c14, warp4)\n",
    "        corr4 = corr4.view(corr4.shape[0], -1, corr4.shape[3], corr4.shape[4])\n",
    "        corr4 = corr4 / c14.shape[1]\n",
    "        corr4 = self.leakyRELU(corr4)\n",
    "        x = torch.cat((corr4, c14, up_flow5, up_feat5), 1)\n",
    "        x = torch.cat((self.conv4_0(x), x),1)\n",
    "        x = torch.cat((self.conv4_1(x), x),1)\n",
    "        x = torch.cat((self.conv4_2(x), x),1)\n",
    "        x = torch.cat((self.conv4_3(x), x),1)\n",
    "        x = torch.cat((self.conv4_4(x), x),1)\n",
    "        flow4 = self.predict_flow4(x)\n",
    "        up_flow4 = self.deconv4(flow4)\n",
    "        up_feat4 = self.upfeat4(x)\n",
    "\n",
    "\n",
    "        warp3 = self.warp(c23, up_flow4*2.5)\n",
    "        corr3 = self.corr(c13, warp3)\n",
    "        corr3 = corr3.view(corr3.shape[0], -1, corr3.shape[3], corr3.shape[4])\n",
    "        corr3 = corr3 / c13.shape[1]\n",
    "        corr3 = self.leakyRELU(corr3)\n",
    "        \n",
    "\n",
    "        x = torch.cat((corr3, c13, up_flow4, up_feat4), 1)\n",
    "        x = torch.cat((self.conv3_0(x), x),1)\n",
    "        x = torch.cat((self.conv3_1(x), x),1)\n",
    "        x = torch.cat((self.conv3_2(x), x),1)\n",
    "        x = torch.cat((self.conv3_3(x), x),1)\n",
    "        x = torch.cat((self.conv3_4(x), x),1)\n",
    "        flow3 = self.predict_flow3(x)\n",
    "        up_flow3 = self.deconv3(flow3)\n",
    "        up_feat3 = self.upfeat3(x)\n",
    "\n",
    "\n",
    "        warp2 = self.warp(c22, up_flow3*5.0)\n",
    "        corr2 = self.corr(c12, warp2)\n",
    "        corr2 = corr2.view(corr2.shape[0], -1, corr2.shape[3], corr2.shape[4])\n",
    "        corr2 = corr2 / c12.shape[1]\n",
    "        corr2 = self.leakyRELU(corr2)\n",
    "        x = torch.cat((corr2, c12, up_flow3, up_feat3), 1)\n",
    "        x = torch.cat((self.conv2_0(x), x),1)\n",
    "        x = torch.cat((self.conv2_1(x), x),1)\n",
    "        x = torch.cat((self.conv2_2(x), x),1)\n",
    "        x = torch.cat((self.conv2_3(x), x),1)\n",
    "        x = torch.cat((self.conv2_4(x), x),1)\n",
    "        flow2 = self.predict_flow2(x)\n",
    "\n",
    "        flow_up = self.upsample1(flow2*self.div_flow)\n",
    "        \n",
    "        flow_preds = [flow2, flow3, flow4, flow5, flow6]\n",
    "\n",
    "        if self.training:\n",
    "            return flow_preds\n",
    "\n",
    "        else:\n",
    "            return flow_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "068e1797-f907-41ba-8c17-3ac69b42a1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptflow_model = PWCNet()\n",
    "# ptflow_model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0017cee5-fbe4-415b-9e70-5fce7a0ffb08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1, img2 = torch.randn(1, 3, 256, 256), torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# img1 = img1.to(device)\n",
    "# img2 = img2.to(device)\n",
    "\n",
    "img1.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e2e320e-2b8f-47aa-b3a5-7d945408f04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flows = ptflow_model(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a53905f2-17f8-463b-a747-a26fa073fc4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 64, 64])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flows[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf7b0e86-ec8d-4322-824f-dfbf55ae75ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ptflow_model.eval()\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d2cec25-9807-4ea8-9264-82762e34d06a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 256, 256])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow = ptflow_model(img1, img2)\n",
    "\n",
    "flow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef9c07c-fc55-4f17-9eda-e55181ba43d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "___\n",
    "\n",
    "## EzFlow PWCNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50b6e53f-1042-4fa5-b3ea-8267655032dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezflow.models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04afdb91-70a4-4d05-b641-069df5382faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prajn\\miniconda3\\envs\\ezflow\\lib\\site-packages\\torch\\nn\\modules\\conv.py:132: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  (out_channels, in_channels // groups, *kernel_size), **factory_kwargs))\n"
     ]
    }
   ],
   "source": [
    "ezflow_model = build_model('PWCNet', cfg_path='../configs/pwcnet/models/ezflow.yaml', custom_cfg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb4cf8d-b233-4905-93c1-54ee41b03038",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prajn\\miniconda3\\envs\\ezflow\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "flows = ezflow_model(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7157577-5b94-41f9-aa8b-d9f4914755ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 64, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flows[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e23172bd-9795-4172-bfec-c8e05ca15db9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 256, 256])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ezflow_model.eval()\n",
    "\n",
    "flow = ezflow_model(img1, img2)\n",
    "flow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263ccc6a",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### nnflow PWCNetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a534c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnflow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82039151",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnflow_model = build_model('PWCNetV1', cfg_path='../configs/pwcnet/models/nnflow_v1.yaml', custom_cfg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cfd1b0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 20.0, 0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnflow_model.md, nnflow_model.div_flow, nnflow_model.padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3902f31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 64, 64])\n",
      "torch.Size([1, 2, 32, 32])\n",
      "torch.Size([1, 2, 16, 16])\n",
      "torch.Size([1, 2, 8, 8])\n",
      "torch.Size([1, 2, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "nnflow_model.train()\n",
    "flows = nnflow_model(img1, img2)\n",
    "\n",
    "\n",
    "for flow in flows:\n",
    "    print(flow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e6ee4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 256, 256])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnflow_model.eval()\n",
    "\n",
    "flow_up, flow_preds_v1 = nnflow_model(img1, img2)\n",
    "flow_up.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2705135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flow_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2bb92f",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### nnflow PWCNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "43437897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nnflow_model_v2 = build_model('PWCNetV2', cfg_path='../configs/pwcnet/models/nnflow_v2.yaml', custom_cfg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "749f2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnflow_model_v3 = build_model('PWCNetV2', cfg_path='../configs/pwcnet/models/nnflow_v3.yaml', custom_cfg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26c628af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "flows = nnflow_model_v3(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2fc8f94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level6 torch.Size([1, 2, 4, 4])\n",
      "level5 torch.Size([1, 2, 8, 8])\n",
      "level4 torch.Size([1, 2, 16, 16])\n",
      "level3 torch.Size([1, 2, 32, 32])\n",
      "level2 torch.Size([1, 2, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "for level in flows:\n",
    "    print(f\"{level} {flows[level].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e135e2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 256, 256])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnflow_model_v3.eval()\n",
    "\n",
    "flow_up, flow_preds_v3 = nnflow_model_v3(img1, img2)\n",
    "flow_up.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7a032",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c9e6d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.randn(1,2,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57bcde7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezflow.functional import FUNCTIONAL_REGISTRY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7c437ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nnflow.losses.MultiLevelEPE"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = FUNCTIONAL_REGISTRY.get('MultiLevelEPE')\n",
    "# loss = FUNCTIONAL_REGISTRY.get('MultiScale')\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e8460cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b6718816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7070, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(flow_preds_v1, target / 20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e8d9c5e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4985, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn(flow_preds_v3, target / 20.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e8ccb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
