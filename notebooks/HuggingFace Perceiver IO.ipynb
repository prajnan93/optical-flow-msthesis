{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "420cb447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d06671b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezflow.models import MODEL_REGISTRY\n",
    "from ezflow.config import configurable\n",
    "from ezflow.modules import BaseModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce629cbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30c335",
   "metadata": {},
   "source": [
    "___\n",
    "1. https://huggingface.co/blog/perceiver\n",
    "2. https://huggingface.co/docs/transformers/v4.21.3/en/model_doc/perceiver\n",
    "___\n",
    "3. https://huggingface.co/docs/transformers/v4.21.3/en/model_doc/perceiver#transformers.PerceiverForOpticalFlow\n",
    "4. https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/perceiver/modeling_perceiver.py#L1612\n",
    "5. https://github.com/NielsRogge/Transformers-Tutorials/blob/master/Perceiver/Perceiver_for_Optical_Flow.ipynb\n",
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c62a218b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PerceiverModel, PerceiverConfig\n",
    "from transformers import PerceiverForOpticalFlow\n",
    "from transformers.models.perceiver.modeling_perceiver import PerceiverImagePreprocessor, PerceiverTrainablePositionEncoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed61fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bef138a",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bb7db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches = torch.randn(1,2,27,368, 496)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e832b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = PerceiverForOpticalFlow.from_pretrained(\"deepmind/optical-flow-perceiver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "986bea04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41057134"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcb04bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_model = pretrained_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b222ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# patches = patches.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "254ff09c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f458eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = pretrained_model(inputs=patches, return_dict=False)\n",
    "\n",
    "flow = outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ef44a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 368, 496, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2c6d0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pretrained_model, flow, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d6a93d",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "245cbfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceiver(BaseModule):\n",
    "    \"\"\"\n",
    "    Implementation of PerceiverIO Optical Flow\n",
    "    https://www.deepmind.com/open-source/perceiver-io\n",
    "    https://huggingface.co/docs/transformers/v4.21.3/en/model_doc/perceiver#transformers.PerceiverForOpticalFlow\n",
    "    https://github.com/huggingface/transformers/blob/v4.21.3/src/transformers/models/perceiver/modeling_perceiver.py#L1612\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    cfg : :class:`CfgNode`\n",
    "        Configuration for the model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg):\n",
    "        super(Perceiver, self).__init__()\n",
    "        \n",
    "        self.config = PerceiverConfig(**cfg)\n",
    "        \n",
    "        fourier_position_encoding_kwargs_preprocessor = dict(\n",
    "            num_bands=64,\n",
    "            max_resolution=self.config.train_size,\n",
    "            sine_only=False,\n",
    "            concat_pos=True,\n",
    "        )\n",
    "        fourier_position_encoding_kwargs_decoder = dict(\n",
    "            concat_pos=True, max_resolution=self.config.train_size, num_bands=64, sine_only=False\n",
    "        )\n",
    "        \n",
    "        image_preprocessor = PerceiverImagePreprocessor(\n",
    "            self.config,\n",
    "            prep_type=\"patches\",\n",
    "            spatial_downsample=1,\n",
    "            conv_after_patching=True,\n",
    "            conv_after_patching_in_channels=54,\n",
    "            temporal_downsample=2,\n",
    "            position_encoding_type=\"fourier\",\n",
    "            # position_encoding_kwargs\n",
    "            fourier_position_encoding_kwargs=fourier_position_encoding_kwargs_preprocessor,\n",
    "        )\n",
    "        \n",
    "        self.perceiver = PerceiverModel(\n",
    "            self.config,\n",
    "            input_preprocessor=image_preprocessor,\n",
    "            decoder=PerceiverOpticalFlowDecoder(\n",
    "                self.config,\n",
    "                num_channels=image_preprocessor.num_channels,\n",
    "                output_image_shape=self.config.train_size,\n",
    "                rescale_factor=100.0,\n",
    "                use_query_residual=False,\n",
    "                output_num_channels=2,\n",
    "                position_encoding_type=\"fourier\",\n",
    "                fourier_position_encoding_kwargs=fourier_position_encoding_kwargs_decoder,\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        self._init_weights()\n",
    "        \n",
    "    def _init_weights(self):\n",
    "        \n",
    "        for module in self.modules():\n",
    "            if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "                # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "                # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif hasattr(module, \"latents\"):\n",
    "                module.latents.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            elif hasattr(module, \"position_embeddings\") and isinstance(module, PerceiverTrainablePositionEncoding):\n",
    "                module.position_embeddings.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            elif isinstance(module, nn.ParameterDict):\n",
    "                for modality in module.keys():\n",
    "                    module[modality].data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            elif isinstance(module, nn.Embedding):\n",
    "                module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "                if module.padding_idx is not None:\n",
    "                    module.weight.data[module.padding_idx].zero_()\n",
    "            elif isinstance(module, nn.LayerNorm):\n",
    "                module.bias.data.zero_()\n",
    "                module.weight.data.fill_(1.0)\n",
    "        \n",
    "    # source: https://discuss.pytorch.org/t/tf-extract-image-patches-in-pytorch/43837/9\n",
    "    def _extract_image_patches(self, x, kernel=3, stride=1, dilation=1):\n",
    "        # Do TF 'SAME' Padding\n",
    "        b,c,h,w = x.shape\n",
    "        h2 = math.ceil(h / stride)\n",
    "        w2 = math.ceil(w / stride)\n",
    "        pad_row = (h2 - 1) * stride + (kernel - 1) * dilation + 1 - h\n",
    "        pad_col = (w2 - 1) * stride + (kernel - 1) * dilation + 1 - w\n",
    "        x = F.pad(x, (pad_row//2, pad_row - pad_row//2, pad_col//2, pad_col - pad_col//2))\n",
    "\n",
    "        # Extract patches\n",
    "        patches = x.unfold(2, kernel, stride).unfold(3, kernel, stride)\n",
    "        patches = patches.permute(0,4,5,1,2,3).contiguous()\n",
    "        \n",
    "        return patches.view(b,-1,patches.shape[-2], patches.shape[-1])\n",
    "        \n",
    "    def forward(self, img1, img2):\n",
    "        \n",
    "        B, C, H, W = img1.shape\n",
    "        \n",
    "        patches = self._extract_image_patches(torch.concat([img1, img2], dim=0))\n",
    "        _, C, H, W = patches.shape\n",
    "        patches = patches.view(B, -1, C, H, W)\n",
    "        \n",
    "        flow = self.perceiver(\n",
    "            inputs=patches,\n",
    "            return_dict=False\n",
    "        )[0]\n",
    "        \n",
    "        flow = flow.permute(0, 3, 1, 2)\n",
    "        \n",
    "        output = {\"flow_preds\": flow}\n",
    "        \n",
    "        if self.training:\n",
    "            return output\n",
    "        \n",
    "        output[\"flow_upsampled\"] = flow\n",
    "        \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "807340ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = {\n",
    "      \"_name_or_path\": \"deepmind/optical-flow-perceiver\",\n",
    "      \"architectures\": [\n",
    "        \"PerceiverForOpticalFlow\"\n",
    "      ],\n",
    "      \"attention_probs_dropout_prob\": 0.1,\n",
    "      \"audio_samples_per_frame\": 1920,\n",
    "      \"cross_attention_shape_for_attention\": \"kv\",\n",
    "      \"cross_attention_widening_factor\": 1,\n",
    "      \"d_latents\": 512,\n",
    "      \"d_model\": 322,\n",
    "      \"hidden_act\": \"gelu\",\n",
    "      \"hidden_dropout_prob\": 0.1,\n",
    "      \"image_size\": 56,\n",
    "      \"initializer_range\": 0.02,\n",
    "      \"layer_norm_eps\": 1e-12,\n",
    "      \"max_position_embeddings\": 2048,\n",
    "      \"model_type\": \"perceiver\",\n",
    "      \"num_blocks\": 1,\n",
    "      \"num_cross_attention_heads\": 1,\n",
    "      \"num_frames\": 16,\n",
    "      \"num_latents\": 2048,\n",
    "      \"num_self_attends_per_block\": 24,\n",
    "      \"num_self_attention_heads\": 16,\n",
    "      \"output_shape\": [\n",
    "        1,\n",
    "        16,\n",
    "        224,\n",
    "        224\n",
    "      ],\n",
    "      \"qk_channels\": None,\n",
    "      \"samples_per_patch\": 16,\n",
    "      \"self_attention_widening_factor\": 1,\n",
    "      \"seq_len\": 2048,\n",
    "      \"torch_dtype\": \"float32\",\n",
    "      \"train_size\": [\n",
    "        368,\n",
    "        496\n",
    "      ],\n",
    "      \"transformers_version\": \"4.21.3\",\n",
    "      \"use_query_residual\": True,\n",
    "      \"v_channels\": None,\n",
    "      \"vocab_size\": 262\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "79b0d89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Perceiver(config_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f622b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41057134"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "197d77da",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = torch.randn(1, 3, 368, 496)\n",
    "img2 = torch.randn(1, 3, 368, 496)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4fc939bc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prajn\\miniconda3\\envs\\ezflow\\lib\\site-packages\\torch\\functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ..\\aten\\src\\ATen\\native\\TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "output = model(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd6da2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 368, 496])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow = output[\"flow_preds\"]\n",
    "flow.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f26701",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "### Build using Ezflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "346fe39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezflow.models import build_model\n",
    "from nnflow import Perceiver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44f4e45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model = build_model('Perceiver', cfg_path='../configs/perceiver/models/perceiver.yaml', custom_cfg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6fa06276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_name_or_path': 'deepmind/optical-flow-perceiver',\n",
       " 'architectures': ['PerceiverForOpticalFlow'],\n",
       " 'attention_probs_dropout_prob': 0.1,\n",
       " 'audio_samples_per_frame': 1920,\n",
       " 'cross_attention_shape_for_attention': 'kv',\n",
       " 'cross_attention_widening_factor': 1,\n",
       " 'd_latents': 512,\n",
       " 'd_model': 322,\n",
       " 'hidden_act': 'gelu',\n",
       " 'hidden_dropout_prob': 0.1,\n",
       " 'image_size': 56,\n",
       " 'initializer_range': 0.02,\n",
       " 'layer_norm_eps': 1e-12,\n",
       " 'max_position_embeddings': 2048,\n",
       " 'model_type': 'perceiver',\n",
       " 'num_blocks': 1,\n",
       " 'num_cross_attention_heads': 1,\n",
       " 'num_frames': 16,\n",
       " 'num_latents': 2048,\n",
       " 'num_self_attends_per_block': 24,\n",
       " 'num_self_attention_heads': 16,\n",
       " 'output_shape': [1, 16, 224, 224],\n",
       " 'qk_channels': None,\n",
       " 'samples_per_patch': 16,\n",
       " 'self_attention_widening_factor': 1,\n",
       " 'seq_len': 2048,\n",
       " 'torch_dtype': 'float32',\n",
       " 'train_size': [368, 496],\n",
       " 'transformers_version': '4.21.3',\n",
       " 'use_query_residual': True,\n",
       " 'v_channels': None,\n",
       " 'vocab_size': 262}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_model.cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad7f461",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = torch.randn(1, 3, 368, 496)\n",
    "img2 = torch.randn(1, 3, 368, 496)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9809b16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img1 = img1.to(device)\n",
    "# img2 = img2.to(device)\n",
    "# _model = _model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d527e6b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\CPUAllocator.cpp:76] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1495269376 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19672\\1061548997.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mflow_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\nnflow-0.0.0-py3.7.egg\\nnflow\\perceiver_io.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img1, img2)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         perceiver_outputs = self.perceiver(\n\u001b[1;32m--> 135\u001b[1;33m             \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpatches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         )\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\transformers\\models\\perceiver\\modeling_perceiver.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs, attention_mask, subsampled_output_points, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    916\u001b[0m                 \u001b[0mz\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    917\u001b[0m                 \u001b[0mquery_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 918\u001b[1;33m                 \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    919\u001b[0m             )\n\u001b[0;32m    920\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\transformers\\models\\perceiver\\modeling_perceiver.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, z, query_mask, output_attentions)\u001b[0m\n\u001b[0;32m   2289\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2290\u001b[0m     ) -> PerceiverDecoderOutput:\n\u001b[1;32m-> 2291\u001b[1;33m         \u001b[0mdecoder_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2292\u001b[0m         \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder_outputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2293\u001b[0m         \u001b[1;31m# Output flow and rescale.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\transformers\\models\\perceiver\\modeling_perceiver.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, query, z, query_mask, output_attentions)\u001b[0m\n\u001b[0;32m   2204\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2205\u001b[0m             \u001b[0minputs_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2206\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2207\u001b[0m         )\n\u001b[0;32m   2208\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\transformers\\models\\perceiver\\modeling_perceiver.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, inputs, inputs_mask, output_attentions)\u001b[0m\n\u001b[0;32m    470\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[0minputs_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 472\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    473\u001b[0m         )\n\u001b[0;32m    474\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattention_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\transformers\\models\\perceiver\\modeling_perceiver.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, inputs, inputs_mask, output_attentions)\u001b[0m\n\u001b[0;32m    391\u001b[0m             \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m             \u001b[0minputs_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m             \u001b[0moutput_attentions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m         )\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\ezflow\\lib\\site-packages\\transformers\\models\\perceiver\\modeling_perceiver.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, inputs, inputs_mask, output_attentions)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m         \u001b[1;31m# Take the dot product between the queries and keys to get the raw attention scores.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 265\u001b[1;33m         \u001b[0mattention_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq_head_dim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqueries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:76] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1495269376 bytes."
     ]
    }
   ],
   "source": [
    "flow_outputs = _model(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7351dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow = flow_outputs[\"flow_preds\"]\n",
    "flow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd2a4912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b693865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_patches(x, kernel=3, stride=1, dilation=1):\n",
    "        # Do TF 'SAME' Padding\n",
    "        b,c,h,w = x.shape\n",
    "        h2 = math.ceil(h / stride)\n",
    "        w2 = math.ceil(w / stride)\n",
    "        pad_row = (h2 - 1) * stride + (kernel - 1) * dilation + 1 - h\n",
    "        pad_col = (w2 - 1) * stride + (kernel - 1) * dilation + 1 - w\n",
    "        x = F.pad(x, (pad_row//2, pad_row - pad_row//2, pad_col//2, pad_col - pad_col//2))\n",
    "\n",
    "        # Extract patches\n",
    "        patches = x.unfold(2, kernel, stride).unfold(3, kernel, stride)\n",
    "        patches = patches.permute(0,4,5,1,2,3).contiguous()\n",
    "\n",
    "        return patches.view(b,-1,patches.shape[-2], patches.shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3a9daea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3, 368, 496])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = torch.stack([img1, img2], dim=1)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6214a9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 368, 496])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b, _, c, h, w = imgs.shape\n",
    "imgs = imgs.view(b*_, c, h, w)\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "509b4590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 27, 368, 496])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = extract_image_patches(imgs)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b0bbb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 18, 3, 368, 496])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = patches.view(b, -1, c, h, w)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf10b46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
