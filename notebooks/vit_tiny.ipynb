{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63ed8584-fb31-430c-963c-077fa058509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "767b621f-beb5-4136-9570-621864bfcf12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d000df-869e-4cb0-b3d5-3d5c0ff204d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return str(sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000000) + \"M params\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d824215-5f83-43ab-8315-bf1710526f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/goswami.p/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "vits8 = torch.hub.load('facebookresearch/dino:main', 'dino_vits8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d218b5bd-5618-4dee-85e5-04d957f1c245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'21.670272M params'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(vits8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "7b66f952-11a7-48ea-a680-cb31bef3ba9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vits8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f8d5f64-36e2-4f3b-8c65-ce7757691480",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_ckpt = torch.load(\"../../dino_vit_pretrained/dino_deitsmall8_pretrain.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c41a42b4-5a4c-4b7a-b57d-ceadc60f172c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vits8.load_state_dict(pretrained_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "44397732-84d6-4f7c-9041-2d0050b6dc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(2, 3, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c28682d8-ebe6-437f-9525-2b417e50bb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = vits8(img)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b3e82305-7086-411d-bc55-2ebde1be337e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vits8.blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1bd782a5-6040-406b-9097-50f6bc6c4524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vits8.blocks) - 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "552d0eac-4d0e-4687-ba44-03cdf981f6c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3073, 384])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_maps = vits8.get_intermediate_layers(img, n=1)\n",
    "feature_maps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "8ee610dc-c833-4a54-bc0f-1b12a4a7fa91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3072, 384])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature = feature_maps[0][:,1:,:]\n",
    "feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3682ecbc-3c5d-46e0-a1f7-a80eb8d3be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ezflow.encoder import ENCODER_REGISTRY\n",
    "from ezflow.config import configurable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4523452e-cd8b-4dd4-88c6-a02ff424ec7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/goswami.p/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "_ENCODER = torch.hub.load('facebookresearch/dino:main', 'dino_vits8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8b547ac6-1bdf-4476-ac4c-302de20a6eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ENCODER_REGISTRY.register()\n",
    "class DinoVITS8(nn.Module):\n",
    "    \"\"\"\n",
    "    This class is a wrapper for the DinoViT model without the classification head\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    @configurable\n",
    "    def __init__(\n",
    "        self,\n",
    "        freeze=True,\n",
    "        pretrained_ckpt_path=None\n",
    "    ):\n",
    "        \n",
    "        super(DinoVITS8, self).__init__()\n",
    "        \n",
    "        self.freeze = freeze\n",
    "        self.feature_extractor = _ENCODER\n",
    "        \n",
    "        if pretrained_ckpt_path is not None:\n",
    "            self.feature_extractor.load_state_dict(\n",
    "                torch.load(pretrained_ckpt_path)\n",
    "            )\n",
    "            print(f\"Loaded Dino ViT S/8 pretrained checkpoint from {pretrained_ckpt_path}\\n\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(self, cfg):\n",
    "        return {\n",
    "            \"freeze\": cfg.FREEZE,\n",
    "            \"pretrained_ckpt_path\": cfg.PRETRAINED_CKPT_PATH\n",
    "        }\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        \"\"\"\n",
    "        _,c,h,w = input.shape\n",
    "\n",
    "        if self.freeze:\n",
    "            self.eval()\n",
    "            self.feature_extractor.eval()\n",
    "        \n",
    "        output = self.feature_extractor.get_intermediate_layers(input, n=1)[0]\n",
    "        \n",
    "        # remove cls token\n",
    "        output = output[:,1:,:]\n",
    "        output = output.permute(0,2,1)\n",
    "\n",
    "        h, w = int(h/8), int(w/8)\n",
    "        b, c, _ = output.shape \n",
    "\n",
    "        output = output.reshape(b, c, h, w)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "13f6f24f-d5ba-4490-8e13-57d915a01ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dino ViT S/8 pretrained checkpoint from ../../dino_vit_pretrained/dino_deitsmall8_pretrain.pth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "encoder = DinoVITS8(\n",
    "    freeze=True, \n",
    "    pretrained_ckpt_path=\"../../dino_vit_pretrained/dino_deitsmall8_pretrain.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "0291c6cc-bfd9-48c3-bc10-9a1d2590de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(2,3,368,496)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1c4e3840-37d2-4118-a000-54004819f475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384, 46, 62])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_map = encoder(img)\n",
    "feat_map.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad0e1fc-e446-4972-9016-4b911be31e53",
   "metadata": {
    "tags": []
   },
   "source": [
    "___\n",
    "## HuggingFace DINO ViT S/8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc2f6a95-d081-4968-bfdd-c825baa475ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, ViTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e84808-41bd-4a5a-a3ea-edd2344ff1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.vit.modeling_vit import ViTEncoder, ViTSelfAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b24dbb5-52cb-474c-9369-d3b87d6a6c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"encoder_stride\": 16,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"vit\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.22.2\"\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration = ViTConfig()\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "098661cf-dc9e-42cb-928b-d664ace146fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration.patch_size=8\n",
    "configuration.hidden_size=128\n",
    "configuration.num_hidden_layers=12\n",
    "configuration.num_attention_heads=4\n",
    "configuration.intermediate_size=768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fea14336-2888-4830-938e-9e37e8d34ba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"encoder_stride\": 16,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 128,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 768,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"vit\",\n",
       "  \"num_attention_heads\": 4,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 8,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.22.2\"\n",
       "}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bd9013e-898e-4410-8937-5edd27540fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = ViTModel(configuration, add_pooling_layer=False)\n",
    "encoder = ViTModel(configuration, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35ef4719-a606-4b5d-af93-07f51d980bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.294336M params'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c4c6b98-c3f2-4f23-bda6-c56b81d4a903",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(2,3,384, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ed2d512-68e0-4931-a11f-f481f5694246",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goswami.p/miniconda3/envs/ezflow/lib/python3.7/site-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
     ]
    }
   ],
   "source": [
    "feats = encoder(img, interpolate_pos_encoding=True).last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b6e43935-6bdb-4b1d-97d0-386cdad40221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3073, 128])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b654201a-faf1-47e9-88ec-8730a484a74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vit_tiny_patch16_224',\n",
       " 'vit_tiny_patch16_224_in21k',\n",
       " 'vit_tiny_patch16_384',\n",
       " 'vit_tiny_r_s16_p8_224',\n",
       " 'vit_tiny_r_s16_p8_224_in21k',\n",
       " 'vit_tiny_r_s16_p8_384']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "timm.list_models(\"vit_tiny*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c7997b53-2424-4af5-8ac2-01f02b18cf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model('vit_tiny_patch16_224')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1385f01f-6b57-4d34-b255-1e28fd2b7c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.717416M params'"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "e0b59210-050e-4277-b737-b4f4fbc758d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embed_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c99294b0-c035-4a5b-acf0-ba677b044ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.models.vision_transformer import _create_vision_transformer as timm_create_vit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "3a7a2cf5-0e9f-43c4-8719-75b343cc6f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm_create_vit('vit_tiny_patch16_224',patch_size=16, embed_dim=128, depth=12, num_heads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "05514bb3-67fb-4d33-9085-3d584782b704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.632296M params'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "621376ff-f665-49d9-89b3-4ba0f1f59ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d0e9e-1f68-4c23-9a68-a66f07f15de7",
   "metadata": {
    "tags": []
   },
   "source": [
    "___\n",
    "## Custom ViT Tiny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f10fb454-aa2d-438b-9a5d-653d8df80a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ezflow.encoder import ENCODER_REGISTRY,build_encoder\n",
    "from ezflow.modules import BaseModule\n",
    "from ezflow.config import configurable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "76dfbf41-7b2a-4bbd-abee-659951e6a2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, ViTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "18dda283-4e85-4556-b307-4871ded11f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @ENCODER_REGISTRY.register()\n",
    "class ViTEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the Swin Transformer without classification head.\n",
    "    \"\"\"\n",
    "\n",
    "    @configurable\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=3,\n",
    "        embedding_channels=96,\n",
    "        depths=(2, 2),\n",
    "        input_resolution=(256, 256),\n",
    "        number_of_heads=(3, 6, 12, 24),\n",
    "        intermediate_size: int = 768,\n",
    "        patch_size: int = 4,\n",
    "        ff_feature_ratio: int = 4,\n",
    "        dropout: float = 0.0,\n",
    "        dropout_attention: float = 0.0,\n",
    "        dropout_path: float = 0.2,\n",
    "        use_checkpoint: bool = False,\n",
    "        sequential_self_attention: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Call super constructor\n",
    "        super(ViTEncoder, self).__init__()\n",
    "\n",
    "        configuration = ViTConfig()\n",
    "        \n",
    "        configuration.patch_size=patch_size\n",
    "        configuration.hidden_size=embedding_channels\n",
    "        configuration.num_hidden_layers=depths\n",
    "        configuration.num_attention_heads=number_of_heads\n",
    "        configuration.intermediate_size=intermediate_size\n",
    "        \n",
    "\n",
    "        self.vit_feature_extractor = ViTModel(configuration, add_pooling_layer=False)\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(self, cfg):\n",
    "        return {\n",
    "            \"in_channels\": cfg.IN_CHANNELS,\n",
    "            \"embedding_channels\": cfg.EMBEDDING_CHANNELS,\n",
    "            \"depths\": cfg.DEPTHS,\n",
    "            \"input_resolution\": cfg.INPUT_RESOLUTION,\n",
    "            \"number_of_heads\": cfg.NUMBER_OF_HEADS,\n",
    "            \"intermediate_size\": cfg.INTERMEDIATE_SIZE,\n",
    "            \"patch_size\": cfg.PATCH_SIZE,\n",
    "            \"ff_feature_ratio\": cfg.FF_FEATURE_RATIO,\n",
    "            \"dropout\": cfg.DROPOUT,\n",
    "            \"dropout_attention\": cfg.DROPOUT_ATTENTION,\n",
    "            \"dropout_path\": cfg.DROPOUT_PATH,\n",
    "            \"use_checkpoint\": cfg.USE_CHECKPOINT,\n",
    "            \"sequential_self_attention\": cfg.SEQUENTIAL_SELF_ATTENTION,\n",
    "        }\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        \"\"\"\n",
    "        _,c,h,w = input.shape\n",
    "\n",
    "        output = self.vit_feature_extractor(input, interpolate_pos_encoding=True).last_hidden_state\n",
    "        \n",
    "        # remove cls token\n",
    "        output = output[:,1:,:]\n",
    "        output = output.permute(0,2,1)\n",
    "\n",
    "        h, w = int(h/8), int(w/8)\n",
    "        b, c, _ = output.shape \n",
    "\n",
    "        output = output.reshape(b, c, h, w)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2a4be18e-42b3-4ef1-ab7a-a36776dc8404",
   "metadata": {},
   "outputs": [],
   "source": [
    "_encoder = ViTEncoder(\n",
    "        patch_size=8,\n",
    "        embedding_channels=128,\n",
    "        depths=12,\n",
    "        number_of_heads=4,\n",
    "        intermediate_size=768\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "54906b9c-4f84-457b-abd6-0277cb7aa82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.294336M params'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e2aeda73-eb9e-4a8f-92ba-369c79a83546",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = torch.randn(2,3,384,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1b8bf591-ce2f-40d8-9480-9d7a8153e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = _encoder(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "06c4c552-674e-4a6a-96b1-5b046c0d6f62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 48, 64])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07140316-10fe-4bf0-982d-0a961655ace4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "20537723-b3ea-4521-a5d2-3884ff9e4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embed = nn.Conv2d(3, 384, kernel_size=(8, 8), stride=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d042cf6-80df-47aa-8cbf-bb7a5629fd6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384, 48, 64])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = patch_embed(img)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb882a0b-201b-4040-add2-24cf8df5f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_token = nn.Parameter(\n",
    "            nn.init.trunc_normal_(torch.zeros(1, 1, 384), mean=0.0, std=0.02)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b6b62072-4271-475b-8676-9b059fea56e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 384])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3cae7093-0b3e-4404-94e4-c05beb99c096",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 384])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_tokens = cls_token.expand(2, -1, -1)\n",
    "cls_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a66ed953-d027-47a9-b89c-8d13a65805ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 48, 384])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings.permute(0,3,2,1)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6688a56f-62e6-444b-b5b9-3edbb1121fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3072, 384])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings.reshape(2, 64*48, 384)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9e1d0697-2eb3-4d34-857f-022a61af9ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3073, 384])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ebd7e8f9-4c97-4839-876a-1d30034896d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[[1,1,1],[2,2,2]],[[3,3,3],[4,4,4]]])\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "0008c525-bc8b-47c2-b45f-5eb1e42dcf32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 1, 1],\n",
       "         [2, 2, 2]],\n",
       "\n",
       "        [[3, 3, 3],\n",
       "         [4, 4, 4]]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e94f5bd4-f710-4a87-a2b1-dd42cc747150",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0.]]])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.zeros(2,1,3)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "56fdc7af-55e0-4904-bdc9-ad34e439c2f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.],\n",
       "         [1., 1., 1.],\n",
       "         [2., 2., 2.]],\n",
       "\n",
       "        [[0., 0., 0.],\n",
       "         [3., 3., 3.],\n",
       "         [4., 4., 4.]]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cx = torch.cat((c,x), dim=1)\n",
    "cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e5633b22-75f0-43c0-bd16-54080c6e31f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6a94a33b-7cd2-4b51-9a17-7ee9de7628fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_cx = cx[:,1:,:]\n",
    "_cx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "f8f6998e-774a-4193-b6ba-d63216e6a7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1.],\n",
       "         [2., 2., 2.]],\n",
       "\n",
       "        [[3., 3., 3.],\n",
       "         [4., 4., 4.]]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_cx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "2177d579-f58e-4419-a374-b0db590bf3df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_cx.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fee193-0cd4-4ea1-bfb7-8d4b4cebdb88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
