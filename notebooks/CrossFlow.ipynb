{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f258dc2-bf3f-4a8f-8b0b-1c7b2d248294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# conda environments:\n",
      "#\n",
      "base                     /home/goswami.p/miniconda3\n",
      "cs5330_pa5_1             /home/goswami.p/miniconda3/envs/cs5330_pa5_1\n",
      "dicl                     /home/goswami.p/miniconda3/envs/dicl\n",
      "ezflow                *  /home/goswami.p/miniconda3/envs/ezflow\n",
      "flow                     /home/goswami.p/miniconda3/envs/flow\n",
      "inpaint                  /home/goswami.p/miniconda3/envs/inpaint\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda env list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "21dd74e6-b0d2-4a87-9e20-221e7f0a5850",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ezflow.modules import BaseModule\n",
    "from ezflow.encoder import build_encoder\n",
    "\n",
    "from torch.nn.functional import pad\n",
    "from torch.nn.init import trunc_normal_\n",
    "\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from natten.functional import natten2dqkrpb, natten2dav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf89e5b4-a4a3-4916-bc00-4f776eef563f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnflow.models.gmflow.transformer import FeatureFlowAttention\n",
    "from nnflow.models.gmflow.matching import global_correlation_softmax\n",
    "from nnflow.models.gmflow.utils import feature_add_position\n",
    "from nnflow.models.gmflow.nat import ConvTokenizer, ConvDownsampler, Mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e5b98f-408d-43f1-a7db-810b2a65424f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return str(sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000000) + \"M params\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81b40797-31e1-4507-b377-9bf1967b888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NATLayer with Cross Attention\n",
    "# shortcut in NAT == source in GMFlow Feature Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2ffe8d6-ecb9-44a1-8906-ac8eac91fb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedNeighborhoodAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Neighborhood Attention 2D Module\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, kernel_size, num_heads,\n",
    "                 qkv_bias=True, qk_scale=None, attn_drop=0., proj_drop=0.,\n",
    "                 dilation=None):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = dim // self.num_heads\n",
    "        self.scale = qk_scale or self.head_dim ** -0.5\n",
    "        assert kernel_size > 1 and kernel_size % 2 == 1, \\\n",
    "            f\"Kernel size must be an odd number greater than 1, got {kernel_size}.\"\n",
    "        self.kernel_size = kernel_size\n",
    "        if type(dilation) is str:\n",
    "            self.dilation = None\n",
    "            self.window_size = None\n",
    "        else:\n",
    "            assert dilation is None or dilation >= 1, \\\n",
    "                f\"Dilation must be greater than or equal to 1, got {dilation}.\"\n",
    "            self.dilation = dilation or 1\n",
    "            self.window_size = self.kernel_size * self.dilation\n",
    "\n",
    "        # self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.q_proj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.k_proj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.v_proj = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        \n",
    "        self.rpb = nn.Parameter(torch.zeros(num_heads, (2 * kernel_size - 1), (2 * kernel_size - 1)))\n",
    "        trunc_normal_(self.rpb, std=.02, mean=0., a=-2., b=2.)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, q, k, v):\n",
    "        assert q.shape == k.shape == k.shape\n",
    "        B, Hp, Wp, C = q.shape\n",
    "        H, W = int(Hp), int(Wp)\n",
    "        pad_l = pad_t = pad_r = pad_b = 0\n",
    "        dilation = self.dilation\n",
    "        window_size = self.window_size\n",
    "        if window_size is None:\n",
    "            dilation = max(min(H, W) // self.kernel_size, 1)\n",
    "            window_size = dilation * self.kernel_size\n",
    "        if H < window_size or W < window_size:\n",
    "            pad_l = pad_t = 0\n",
    "            pad_r = max(0, window_size - W)\n",
    "            pad_b = max(0, window_size - H)\n",
    "            q = pad(q, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "            k = pad(k, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "            v = pad(v, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "            _, H, W, _ = q.shape\n",
    "        # qkv = self.qkv(x).reshape(B, H, W, 3, self.num_heads, self.head_dim).permute(3, 0, 4, 1, 2, 5)\n",
    "        # q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        q = self.q_proj(q).reshape(B, H, W, self.num_heads, self.head_dim).permute(0, 3, 1, 2, 4)\n",
    "        k = self.k_proj(k).reshape(B, H, W, self.num_heads, self.head_dim).permute(0, 3, 1, 2, 4)\n",
    "        v = self.v_proj(v).reshape(B, H, W, self.num_heads, self.head_dim).permute(0, 3, 1, 2, 4)\n",
    "        \n",
    "        # print(q.shape, k.shape, v.shape)\n",
    "        q = q * self.scale\n",
    "        attn = natten2dqkrpb(q, k, self.rpb, dilation)\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        x = natten2dav(attn, v, dilation)\n",
    "        x = x.permute(0, 2, 3, 1, 4).reshape(B, H, W, C)\n",
    "        if pad_r or pad_b:\n",
    "            x = x[:, :Hp, :Wp, :]\n",
    "\n",
    "        return self.proj_drop(self.proj(x))\n",
    "\n",
    "    def extra_repr(self) -> str:\n",
    "        return f'kernel_size={self.kernel_size}, dilation={self.dilation}, head_dim={self.head_dim}, num_heads={self.num_heads}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50e4a907-df3a-4786-af89-2b756bd56cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embed = ConvTokenizer(in_chans=3, embed_dim=64, norm_layer=nn.LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0df7cf03-4d4c-40f1-bbfb-8ab28489cf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn = ModifiedNeighborhoodAttention(\n",
    "    dim=64,\n",
    "    kernel_size=7,\n",
    "    num_heads=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc9d5053-02a3-4792-844b-4b2ab2aa98da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,3,256,256)\n",
    "out = patch_embed(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5382a00a-ec2b-4150-b8e7-aab551eaf00f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 64])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = attn(out, out, out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f715dbbc-28b0-4cd3-ac89-8c3b6f798a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModifiedNeighborhoodAttention(\n",
       "  kernel_size=7, dilation=1, head_dim=32, num_heads=2\n",
       "  (q_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (k_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (v_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "  (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (proj_drop): Dropout(p=0.0, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3a875c-ec1c-4be8-94d0-334e11bba330",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "747de078-2af3-4e85-8e5e-77c0716c7f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c47bb8a4-e192-4aa6-945f-b55cfef9e910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedNATLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        num_heads,\n",
    "        kernel_size=7,\n",
    "        dilation=None,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        act_layer=nn.GELU,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        no_ffn=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.no_ffn = no_ffn\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = ModifiedNeighborhoodAttention(\n",
    "            dim,\n",
    "            kernel_size=kernel_size,\n",
    "            dilation=dilation,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            qk_scale=qk_scale,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        \n",
    "        # no ffn after self-attn, with ffn after cross-attn\n",
    "        if not self.no_ffn:\n",
    "            self.norm2 = norm_layer(dim)\n",
    "            self.mlp = Mlp(\n",
    "                in_features=dim,\n",
    "                hidden_features=int(dim * mlp_ratio),\n",
    "                act_layer=act_layer,\n",
    "                drop=drop,\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        shortcut = source\n",
    "\n",
    "        x = torch.cat([source, target], dim=0)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        source, target = x.chunk(chunks=2, dim=0)   \n",
    "        query, key, value = source, target, target\n",
    "\n",
    "        x = self.attn(query, key, value)\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        \n",
    "        # no ffn after self-attn, with ffn after cross-attn\n",
    "        if not self.no_ffn:\n",
    "            x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "            \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d00d8f58-f232-4a32-a036-048fc2d6ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_layer = ModifiedNATLayer(\n",
    "    dim=64,\n",
    "    num_heads=2,\n",
    "    mlp_ratio=3.0, \n",
    "    no_ffn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fc40391-29ac-4fd8-b606-2eb23e4dae02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModifiedNATLayer(\n",
       "  (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "  (attn): ModifiedNeighborhoodAttention(\n",
       "    kernel_size=7, dilation=1, head_dim=32, num_heads=2\n",
       "    (q_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (k_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (v_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "    (proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (drop_path): Identity()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9712fa38-9cc1-46b7-bdab-b2086fcc452a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 64])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,3,256,256)\n",
    "out = patch_embed(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "afed43cd-8538-492e-8dfd-6d975e926236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = attn_layer(source=out, target=out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96330207-6c8e-4bbf-b64a-f1292992ef2f",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7398bf0-046f-4893-9835-b351c557a273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Transformer Block with alternating Self-Cross layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81a40a09-0fa3-4fbb-927e-7cecb219d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedNATBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        num_heads,\n",
    "        kernel_size,\n",
    "        dilations=None,\n",
    "        downsample=True,\n",
    "        mlp_ratio=4.0,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop=0.0,\n",
    "        attn_drop=0.0,\n",
    "        drop_path=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        self_no_ffn=True,\n",
    "        cross_no_ffn=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.depth = depth\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for i in range(depth):\n",
    "            # Self Attention Block\n",
    "            self.blocks.append(\n",
    "                 ModifiedNATLayer(\n",
    "                    dim=dim,\n",
    "                    num_heads=num_heads,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=None if dilations is None else dilations[i],\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i]\n",
    "                    if isinstance(drop_path, list)\n",
    "                    else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                    no_ffn=self_no_ffn\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            # Cross Attention Block\n",
    "            self.blocks.append(\n",
    "                 ModifiedNATLayer(\n",
    "                    dim=dim,\n",
    "                    num_heads=num_heads,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=None if dilations is None else dilations[i],\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    qk_scale=qk_scale,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i]\n",
    "                    if isinstance(drop_path, list)\n",
    "                    else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                    no_ffn=cross_no_ffn\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        self.downsample = (\n",
    "            None if not downsample else ConvDownsampler(dim=dim, norm_layer=norm_layer)\n",
    "        )\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        for i in range(len(self.blocks)):\n",
    "            \n",
    "            # self attention\n",
    "            if i % 2 == 0:\n",
    "                source = self.blocks[i](source, source)\n",
    "                \n",
    "            # cross attention and feed forward\n",
    "            else:\n",
    "                source = self.blocks[i](source, target)\n",
    "            \n",
    "        if self.downsample is None:\n",
    "            return source\n",
    "        \n",
    "        return self.downsample(source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6aa3cbaa-4129-407d-b61a-62963f967aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_blk = ModifiedNATBlock(\n",
    "    dim=64,\n",
    "    depth=3,\n",
    "    num_heads=2,\n",
    "    kernel_size=7,\n",
    "    mlp_ratio=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed43ecfe-9e39-4a6b-9b8a-7ce2f8bb4d92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2515M params'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(transformer_blk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "033f8f24-3018-48cd-b04a-a4d2d5820622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128, 64])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1,3,512,512)\n",
    "out = patch_embed(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "846e5e50-b788-4c26-891c-ec4bdac087a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64, 128])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = transformer_blk(source=out, target=out)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c6746d-4c97-4e07-a156-62e134c51b47",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a1a3a1b-071e-4af1-8a9c-ccea9922bdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2f98c7a-87f8-4df7-9ea8-db57648392f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedNAT(nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim,\n",
    "        mlp_ratio,\n",
    "        depths,\n",
    "        num_heads,\n",
    "        drop_path_rate=0.2,\n",
    "        in_chans=3,\n",
    "        kernel_size=7,\n",
    "        dilations=None,\n",
    "        qkv_bias=True,\n",
    "        qk_scale=None,\n",
    "        drop_rate=0.0,\n",
    "        attn_drop_rate=0.0,\n",
    "        norm_layer=nn.LayerNorm,\n",
    "        self_no_ffn=True,\n",
    "        cross_no_ffn=False,\n",
    "        use_sine_pos_embed=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "        if isinstance(dilations, str) and dilations == 'None':\n",
    "            dilations = None \n",
    "\n",
    "        self.num_levels = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_levels - 1))\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.use_sine_pos_embed = use_sine_pos_embed\n",
    "\n",
    "        self.patch_embed = ConvTokenizer(\n",
    "            in_chans=in_chans, embed_dim=embed_dim, norm_layer=norm_layer\n",
    "        )\n",
    "\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        self.levels = nn.ModuleList()\n",
    "        for i in range(self.num_levels):\n",
    "            level = ModifiedNATBlock(\n",
    "                dim=int(embed_dim * 2**i),\n",
    "                depth=depths[i],\n",
    "                num_heads=num_heads[i],\n",
    "                kernel_size=kernel_size,\n",
    "                dilations=None if dilations is None else dilations[i],\n",
    "                mlp_ratio=self.mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                qk_scale=qk_scale,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                drop_path=dpr[sum(depths[:i]) : sum(depths[: i + 1])],\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=(i < self.num_levels - 1),\n",
    "                self_no_ffn=self_no_ffn,\n",
    "                cross_no_ffn=cross_no_ffn\n",
    "            )\n",
    "            self.levels.append(level)\n",
    "\n",
    "        self.norm = norm_layer(self.num_features)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=0.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay_keywords(self):\n",
    "        return {\"rpb\"}\n",
    "\n",
    "    def forward_features(self, img1, img2):\n",
    "        b, c, h, w =  img1.shape\n",
    "        \n",
    "        x = self.patch_embed(torch.cat([img1,img2], dim=0))  \n",
    "        x = self.pos_drop(x)\n",
    "        \n",
    "        img1, img2 = x.chunk(chunks=2, dim=0)\n",
    "        \n",
    "        if self.use_sine_pos_embed:\n",
    "            img1, img2 = feature_add_position(img1, img2, attn_splits=0, feature_channels=self.embed_dim)\n",
    "        \n",
    "        # Concat img1 and img2 in batch dimension to compute in parallel\n",
    "        concat1 = torch.cat([img1, img2], dim=0) # 2B, H, W, C \n",
    "        concat2 = torch.cat([img2, img1], dim=0) # 2B, H, W, C\n",
    "        \n",
    "        for level in self.levels:\n",
    "            concat1 = level(concat1, concat2)\n",
    "            \n",
    "            # update feature2\n",
    "            concat2 = torch.cat(concat1.chunk(chunks=2,dim=0)[::-1], dim=0)\n",
    "            \n",
    "        concat1 = self.norm(concat1)\n",
    "        \n",
    "        feature1, feature2 = concat1.chunk(chunks=2,dim=0) # B, H, W, C      \n",
    "        return feature1, feature2\n",
    "\n",
    "    def forward(self, img1, img2):\n",
    "        \n",
    "\n",
    "        feature1, feature2 = self.forward_features(img1, img2)\n",
    "        \n",
    "        feature1 = feature1.permute(0,3,1,2)\n",
    "        feature2 = feature2.permute(0,3,1,2)\n",
    "\n",
    "        return feature1, feature2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ebc66b03-596a-4aec-8f19-44c7118ebd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nat = ModifiedNAT(\n",
    "    embed_dim=64,\n",
    "    mlp_ratio=3,\n",
    "    depths=[3,4],\n",
    "    num_heads=[2,4],\n",
    "    drop_path_rate=0.2,\n",
    "    in_chans=3,\n",
    "    kernel_size=7,\n",
    "    use_sine_pos_embed=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0f54e91e-7a17-4241-be0b-05e52a4604d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.203404M params'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(nat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "197ffe74-9dfb-4c76-9b80-ab15444742e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 32, 32]), torch.Size([1, 128, 32, 32]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = torch.randn(1,3,256,256)\n",
    "img2 = torch.randn(1,3,256,256)\n",
    "\n",
    "feat1, feat2 = nat(img1, img2)\n",
    "\n",
    "feat1.shape, feat2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7715ec2-c952-4741-8849-0871ca218bf5",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6fca1518-858f-4b5c-b364-b07e5aa9bc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New End 2 End Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "371ef7de-4255-499f-a1c4-e8cabd38da3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossFlow(BaseModule):\n",
    "    def __init__(self, cfg):\n",
    "        \n",
    "        super(CrossFlow, self).__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.num_scales = cfg.MODEL.NUM_SCALES\n",
    "        self.feature_channels = cfg.MODEL.FEATURE_CHANNELS\n",
    "        self.upsample_factor = cfg.MODEL.UPSAMPLE_FACTOR\n",
    "        self.num_head = cfg.MODEL.NUM_HEADS\n",
    "        self.attention_type = cfg.MODEL.ATTENTION_TYPE\n",
    "        self.ffn_dim_expansion = cfg.MODEL.FFN_DIM_EXPANSION\n",
    "        self.num_transformer_layers = cfg.MODEL.NUM_TRANSFORMER_LAYERS\n",
    "\n",
    "        self.attn_splits_list=cfg.MODEL.ATTN_SPLITS_LIST\n",
    "        self.corr_radius_list=cfg.MODEL.CORR_RADIUS_LIST\n",
    "        self.prop_radius_list=cfg.MODEL.PROP_RADIUS_LIST\n",
    "        self.pred_bidir_flow=cfg.MODEL.PRED_BIDIR_FLOW\n",
    "        \n",
    "\n",
    "        # Transformer Backbone with alternating self attention and cross attention\n",
    "        self.backbone = ModifiedNAT(\n",
    "            embed_dim=cfg.ENCODER.EMBED_DIMS,\n",
    "            mlp_ratio=cfg.ENCODER.MLP_RATIO,\n",
    "            depths=cfg.ENCODER.DEPTHS,\n",
    "            num_heads=cfg.ENCODER.NUM_HEADS,\n",
    "            drop_path_rate=cfg.ENCODER.DROP_PATH_RATE,\n",
    "            in_chans=cfg.ENCODER.IN_CHANNELS,\n",
    "            kernel_size=cfg.ENCODER.KERNEL_SIZE,\n",
    "            dilations=cfg.ENCODER.DILATIONS,\n",
    "            self_no_ffn=cfg.ENCODER.SELF_NO_FFN,\n",
    "            cross_no_ffn=cfg.ENCODER.CROSS_NO_FFN,\n",
    "            use_sine_pos_embed=cfg.ENCODER.USE_SINE_POS_EMBED\n",
    "        )\n",
    "\n",
    "        # flow propagation with self-attn\n",
    "        self.feature_flow_attn = FeatureFlowAttention(in_channels=self.feature_channels)\n",
    "\n",
    "        # convex upsampling: concat feature0 and flow as input\n",
    "        self.upsampler = nn.Sequential(nn.Conv2d(2 + self.feature_channels, 256, 3, 1, 1),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(256, self.upsample_factor ** 2 * 9, 1, 1, 0))\n",
    "\n",
    "    \n",
    "    def upsample_flow(self, flow, feature, bilinear=False, upsample_factor=8,\n",
    "                      ):\n",
    "        if bilinear:\n",
    "            up_flow = F.interpolate(flow, scale_factor=upsample_factor,\n",
    "                                    mode='bilinear', align_corners=True) * upsample_factor\n",
    "\n",
    "        else:\n",
    "            # convex upsampling\n",
    "            concat = torch.cat((flow, feature), dim=1)\n",
    "\n",
    "            mask = self.upsampler(concat)\n",
    "            b, flow_channel, h, w = flow.shape\n",
    "            mask = mask.view(b, 1, 9, self.upsample_factor, self.upsample_factor, h, w)  # [B, 1, 9, K, K, H, W]\n",
    "            mask = torch.softmax(mask, dim=2)\n",
    "\n",
    "            up_flow = F.unfold(self.upsample_factor * flow, [3, 3], padding=1)\n",
    "            up_flow = up_flow.view(b, flow_channel, 9, 1, 1, h, w)  # [B, 2, 9, 1, 1, H, W]\n",
    "\n",
    "            up_flow = torch.sum(mask * up_flow, dim=2)  # [B, 2, K, K, H, W]\n",
    "            up_flow = up_flow.permute(0, 1, 4, 2, 5, 3)  # [B, 2, K, H, K, W]\n",
    "            up_flow = up_flow.reshape(b, flow_channel, self.upsample_factor * h,\n",
    "                                      self.upsample_factor * w)  # [B, 2, K*H, K*W]\n",
    "\n",
    "        return up_flow\n",
    "\n",
    "    def forward(self, img0, img1):\n",
    "\n",
    "        results_dict = {}\n",
    "        flow_preds = []\n",
    "\n",
    "        # extract features\n",
    "        feature0, feature1 = self.backbone(img0, img1)          \n",
    "\n",
    "        assert len(self.attn_splits_list) == len(self.corr_radius_list) == len(self.prop_radius_list) == self.num_scales\n",
    "\n",
    "\n",
    "        upsample_factor = self.upsample_factor\n",
    "\n",
    "        attn_splits = self.attn_splits_list[0]\n",
    "        corr_radius = self.corr_radius_list[0]\n",
    "        prop_radius = self.prop_radius_list[0]\n",
    "\n",
    "            \n",
    "        # Global matching correlation and softmax\n",
    "        # when predicting bidirectional flow, flow is the \n",
    "        # concatenation of forward flow and backward flow in batch dim [2*B,2,H,W]\n",
    "        flow = global_correlation_softmax(feature0, feature1, self.pred_bidir_flow)[0]\n",
    "        \n",
    "\n",
    "        # upsample to the original resolution for supervison\n",
    "        if self.training:  # only need to upsample intermediate flow predictions at training time\n",
    "            flow_bilinear = self.upsample_flow(flow, None, bilinear=True, upsample_factor=upsample_factor)\n",
    "            flow_preds.append(flow_bilinear)\n",
    "\n",
    "        # flow propagation with self-attn\n",
    "        if self.pred_bidir_flow:\n",
    "            feature0 = torch.cat((feature0, feature1), dim=0)  # [2*B, C, H, W] for propagation\n",
    "            \n",
    "        flow = self.feature_flow_attn(feature0, flow.detach(),\n",
    "                                      local_window_attn=prop_radius > 0,\n",
    "                                      local_window_radius=prop_radius)\n",
    "\n",
    "\n",
    "        flow_up = self.upsample_flow(flow, feature0)\n",
    "        flow_preds.append(flow_up)\n",
    "\n",
    "        results_dict.update({'flow_preds': flow_preds})\n",
    "\n",
    "        if not self.training:\n",
    "            results_dict[\"flow_upsampled\"] = results_dict[\"flow_preds\"][0]\n",
    "\n",
    "        return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "70bcbc6c-1927-47fb-9e2d-4b42513daac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezflow.engine import get_training_cfg as get_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "1b6c9fd9-8a46-4cdc-8b0d-099fef57044c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CfgNode({'NAME': 'GMFlowV2', 'ENCODER': CfgNode({'NAME': 'NAT', 'IN_CHANNELS': 3, 'DEPTHS': [3, 4], 'NUM_HEADS': [2, 4], 'EMBED_DIMS': 64, 'MLP_RATIO': 3, 'DROP_PATH_RATE': 0.2, 'KERNEL_SIZE': 7, 'DILATIONS': 'None', 'USE_SINE_POS_EMBED': True, 'SELF_NO_FFN': True, 'CROSS_NO_FFN': False}), 'MODEL': CfgNode({'FEATURE_CHANNELS': 128, 'NUM_SCALES': 1, 'UPSAMPLE_FACTOR': 8, 'NUM_HEADS': 1, 'ATTENTION_TYPE': 'swin', 'FFN_DIM_EXPANSION': 4, 'NUM_TRANSFORMER_LAYERS': 6, 'ATTN_SPLITS_LIST': [2], 'CORR_RADIUS_LIST': [-1], 'PROP_RADIUS_LIST': [-1], 'PRED_BIDIR_FLOW': False})})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg = get_cfg(\"../configs/gmflow/models/crossflow_v01.yaml\")\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "64f41182-ccaf-47a9-846f-57739308c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CrossFlow(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dbeb4661-8766-4630-8235-7cb33574a203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.684236M params'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "304d36dc-f277-462d-9733-cdf77e4de3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 256, 256]), torch.Size([1, 2, 256, 256]))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1 = torch.randn(1,3,256,256)\n",
    "img2 = torch.randn(1,3,256,256)\n",
    "\n",
    "results = model(img1, img2)\n",
    "results['flow_preds'][0].shape, results['flow_preds'][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2fed85-f856-40e3-9cc4-7432940425e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
