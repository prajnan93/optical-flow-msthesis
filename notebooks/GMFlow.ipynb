{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "906cdf7c",
   "metadata": {},
   "source": [
    "#### GMFlow Implementation\n",
    "\n",
    "https://arxiv.org/abs/2111.13680"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb81621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2838ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/goswami.p/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "from nnflow import GMFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f1e818",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezflow.models import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "237822ae-a08b-482d-a0f7-1d34a39ff61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac187c89-f8aa-48d9-81be-ec902800f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model):\n",
    "    return str(sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000000) + \"M params\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc08db9-f487-4684-a9a5-6424075df471",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "___\n",
    "\n",
    "## GMFlow without refinement\n",
    "\n",
    "##### Use `num_scales=1` and `upsample_factor=8` for GMFlow without refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04aaa5cf-8a39-4ccf-84bf-9199e5c70b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model('GMFlow', cfg_path='../configs/gmflow/models/gmflow_v01.yaml', custom_cfg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56adf0a8-8364-424e-b03a-567f812e0d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.680288M params'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de3c6ce4-a185-4d31-8768-180ddfed27ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GMFlow(\n",
    "#     feature_channels=128,\n",
    "#     num_scales=1,\n",
    "#     upsample_factor=8,\n",
    "#     num_head=1,\n",
    "#     attention_type='swin',\n",
    "#     ffn_dim_expansion=4,\n",
    "#     num_transformer_layers=6,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "efd92175-c35c-4139-8d17-5486366a9f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, img2 = torch.randn(1,3,368,496), torch.randn(1,3,368,496)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5900b8b8-246d-432c-b3d8-7be45a5b65a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flow_result = model(img1, img2,\n",
    "#        attn_splits_list=[2],\n",
    "#        corr_radius_list=[-1],\n",
    "#        prop_radius_list=[-1],\n",
    "#    )\n",
    "\n",
    "flow_result = model(img1, img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c43089a8-15c7-4295-8400-7c3d1e2c3878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['flow_preds'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3a7d7c8b-46a0-4156-83af-cc06b38c7ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(flow_result['flow_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d02c3967-d9d7-4eea-bb4d-2c8d3f01e790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 368, 496])\n",
      "torch.Size([1, 2, 368, 496])\n"
     ]
    }
   ],
   "source": [
    "for flow in flow_result['flow_preds']:\n",
    "    print(flow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3831cfd-d843-4ba2-9934-30728f3af20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['flow_preds', 'flow_upsampled'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2, 256, 256]), torch.Size([1, 2, 256, 256]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "flow_result = model(img1, img2)\n",
    "print(flow_result.keys())\n",
    "\n",
    "flow_result['flow_preds'][0].shape, flow_result['flow_upsampled'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd746c-31ee-473b-88b6-c6321fca3505",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "____\n",
    "\n",
    "## GMFlow with Refinement\n",
    "##### Use `num_scales=2` and `upsample_factor=4` for GMFlow with refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4c1f809-155c-4d59-80b1-e779e7802dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_refinement = GMFlow(\n",
    "    feature_channels=128,\n",
    "    num_scales=2,\n",
    "    upsample_factor=4,\n",
    "    num_head=1,\n",
    "    attention_type='swin',\n",
    "    ffn_dim_expansion=4,\n",
    "    num_transformer_layers=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8b0b4ff1-ed15-41af-aa40-81739582ce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "flow_result = model_with_refinement(img1, img2,\n",
    "       attn_splits_list=[2, 8],\n",
    "       corr_radius_list=[-1, 4],\n",
    "       prop_radius_list=[-1, 1],\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed8a2a25-1f50-4d70-a405-c2e93c7eaeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 256, 256])\n",
      "torch.Size([1, 2, 256, 256])\n",
      "torch.Size([1, 2, 256, 256])\n",
      "torch.Size([1, 2, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "for flow in flow_result['flow_preds']:\n",
    "    print(flow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb09410-b10c-4356-a828-46a260fd488e",
   "metadata": {
    "tags": []
   },
   "source": [
    "___\n",
    "\n",
    "## GMFlowV2\n",
    "\n",
    "An end-to-end Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0eecd0a-d561-4329-8654-3931b3f0e1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnflow import GMFlowV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a35840a-69e9-49ab-980a-f604b057d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model('GMFlowV2', cfg_path='../configs/gmflow/models/gmflow_v07.yaml', custom_cfg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5ead6192-be7f-41c8-a9e5-a30eb18bf763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5.34112M params'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f883c32-c2bb-468d-8e4b-43ae4c5ffdad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 368, 496])\n",
      "torch.Size([1, 2, 368, 496])\n"
     ]
    }
   ],
   "source": [
    "img1, img2 = torch.randn(1,3,368,496), torch.randn(1,3,368,496)\n",
    "\n",
    "# img1 = img1.to(device)\n",
    "# img2 = img2.to(device)\n",
    "# model.to(device)\n",
    "\n",
    "output = model(img1, img2)\n",
    "\n",
    "for flow in output['flow_preds']:\n",
    "    print(flow.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87c1ffc-2a6f-42b7-8cf8-cd827f2f4da7",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c54f58ae-d013-44fe-a613-f6e7f24f4053",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-09 13:18:53.532782: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-09 13:18:53.865788: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-10-09 13:18:55.197388: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /shared/centos7/cuda/11.3/lib64:/shared/centos7/nodejs/14.15.4/lib\n",
      "2022-10-09 13:18:55.197519: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /shared/centos7/cuda/11.3/lib64:/shared/centos7/nodejs/14.15.4/lib\n",
      "2022-10-09 13:18:55.197530: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from nnflow.models.gmflow.backbone import CNNEncoder\n",
    "from nnflow.models.gmflow.transformer import FeatureTransformer, FeatureFlowAttention\n",
    "from nnflow.models.gmflow.matching import global_correlation_softmax, local_correlation_softmax\n",
    "from nnflow.models.gmflow.geometry import flow_warp\n",
    "from nnflow.models.gmflow.utils import normalize_img, feature_add_position\n",
    "from nnflow.models.gmflow import SwinEncoderV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61254044-ab04-42d3-85cf-c5c9f2cba509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ezflow.encoder import ENCODER_REGISTRY,build_encoder\n",
    "from ezflow.modules import BaseModule\n",
    "from ezflow.config import get_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fceaa14-1b7a-456e-be44-430977b70b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMFlowV2(BaseModule):\n",
    "    def __init__(self, cfg):\n",
    "        \n",
    "        super(GMFlowV2, self).__init__()\n",
    "\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.num_scales = cfg.MODEL.NUM_SCALES\n",
    "        self.feature_channels = cfg.MODEL.FEATURE_CHANNELS\n",
    "        self.upsample_factor = cfg.MODEL.UPSAMPLE_FACTOR\n",
    "        self.num_head = cfg.MODEL.NUM_HEADS\n",
    "        self.attention_type = cfg.MODEL.ATTENTION_TYPE\n",
    "        self.ffn_dim_expansion = cfg.MODEL.FFN_DIM_EXPANSION\n",
    "        self.num_transformer_layers = cfg.MODEL.NUM_TRANSFORMER_LAYERS\n",
    "\n",
    "        self.attn_splits_list=cfg.MODEL.ATTN_SPLITS_LIST\n",
    "        self.corr_radius_list=cfg.MODEL.CORR_RADIUS_LIST\n",
    "        self.prop_radius_list=cfg.MODEL.PROP_RADIUS_LIST\n",
    "        self.pred_bidir_flow=cfg.MODEL.PRED_BIDIR_FLOW\n",
    "        \n",
    "        self.use_sine_pos_embed=cfg.MODEL.USE_SINE_POS_EMBED\n",
    "\n",
    "        # CNN backbone\n",
    "        self.backbone = build_encoder(cfg.ENCODER)\n",
    "\n",
    "        # Transformer\n",
    "        self.transformer = FeatureTransformer(num_layers=self.num_transformer_layers,\n",
    "                                              d_model=self.feature_channels,\n",
    "                                              nhead=self.num_head,\n",
    "                                              attention_type=self.attention_type,\n",
    "                                              ffn_dim_expansion=self.ffn_dim_expansion,\n",
    "                                              )\n",
    "\n",
    "        # flow propagation with self-attn\n",
    "        self.feature_flow_attn = FeatureFlowAttention(in_channels=self.feature_channels)\n",
    "\n",
    "        # convex upsampling: concat feature0 and flow as input\n",
    "        self.upsampler = nn.Sequential(nn.Conv2d(2 + self.feature_channels, 256, 3, 1, 1),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       nn.Conv2d(256, self.upsample_factor ** 2 * 9, 1, 1, 0))\n",
    "\n",
    "    def extract_feature(self, img0, img1):\n",
    "        concat = torch.cat((img0, img1), dim=0)  # [2B, C, H, W]\n",
    "        features = self.backbone(concat)  # list of [2B, C, H, W], resolution from high to low\n",
    "        \n",
    "        feature0, feature1 = [], []\n",
    "        \n",
    "        chunks = torch.chunk(features, 2, 0)  # tuple\n",
    "        feature0.append(chunks[0])\n",
    "        feature1.append(chunks[1])\n",
    "        \n",
    "        return feature0, feature1\n",
    "\n",
    "        # reverse: resolution from low to high\n",
    "        # features = features[::-1]\n",
    "\n",
    "        # feature0, feature1 = [], []\n",
    "\n",
    "        # for i in range(len(features)):\n",
    "        #     feature = features[i]\n",
    "        #     chunks = torch.chunk(feature, 2, 0)  # tuple\n",
    "        #     feature0.append(chunks[0])\n",
    "        #     feature1.append(chunks[1])\n",
    "\n",
    "        # return feature0, feature1\n",
    "\n",
    "    def upsample_flow(self, flow, feature, bilinear=False, upsample_factor=8,\n",
    "                      ):\n",
    "        if bilinear:\n",
    "            up_flow = F.interpolate(flow, scale_factor=upsample_factor,\n",
    "                                    mode='bilinear', align_corners=True) * upsample_factor\n",
    "\n",
    "        else:\n",
    "            # convex upsampling\n",
    "            concat = torch.cat((flow, feature), dim=1)\n",
    "\n",
    "            mask = self.upsampler(concat)\n",
    "            b, flow_channel, h, w = flow.shape\n",
    "            mask = mask.view(b, 1, 9, self.upsample_factor, self.upsample_factor, h, w)  # [B, 1, 9, K, K, H, W]\n",
    "            mask = torch.softmax(mask, dim=2)\n",
    "\n",
    "            up_flow = F.unfold(self.upsample_factor * flow, [3, 3], padding=1)\n",
    "            up_flow = up_flow.view(b, flow_channel, 9, 1, 1, h, w)  # [B, 2, 9, 1, 1, H, W]\n",
    "\n",
    "            up_flow = torch.sum(mask * up_flow, dim=2)  # [B, 2, K, K, H, W]\n",
    "            up_flow = up_flow.permute(0, 1, 4, 2, 5, 3)  # [B, 2, K, H, K, W]\n",
    "            up_flow = up_flow.reshape(b, flow_channel, self.upsample_factor * h,\n",
    "                                      self.upsample_factor * w)  # [B, 2, K*H, K*W]\n",
    "\n",
    "        return up_flow\n",
    "\n",
    "    def forward(self, img0, img1):\n",
    "\n",
    "        results_dict = {}\n",
    "        flow_preds = []\n",
    "\n",
    "        # img0, img1 = normalize_img(img0, img1)  # [B, 3, H, W]\n",
    "\n",
    "        # resolution low to high\n",
    "        feature0_list, feature1_list = self.extract_feature(img0, img1)  # list of features        \n",
    "        flow = None\n",
    "\n",
    "        assert len(self.attn_splits_list) == len(self.corr_radius_list) == len(self.prop_radius_list) == self.num_scales\n",
    "\n",
    "        for scale_idx in range(self.num_scales):\n",
    "            feature0, feature1 = feature0_list[scale_idx], feature1_list[scale_idx]\n",
    "\n",
    "            if self.pred_bidir_flow and scale_idx > 0:\n",
    "                # predicting bidirectional flow with refinement\n",
    "                feature0, feature1 = torch.cat((feature0, feature1), dim=0), torch.cat((feature1, feature0), dim=0)\n",
    "\n",
    "            upsample_factor = self.upsample_factor * (2 ** (self.num_scales - 1 - scale_idx))\n",
    "\n",
    "            if scale_idx > 0:\n",
    "                flow = F.interpolate(flow, scale_factor=2, mode='bilinear', align_corners=True) * 2\n",
    "\n",
    "            if flow is not None:\n",
    "                flow = flow.detach()\n",
    "                feature1 = flow_warp(feature1, flow)  # [B, C, H, W]\n",
    "\n",
    "            attn_splits = self.attn_splits_list[scale_idx]\n",
    "            corr_radius = self.corr_radius_list[scale_idx]\n",
    "            prop_radius = self.prop_radius_list[scale_idx]\n",
    "\n",
    "            # add position to features\n",
    "            if self.use_sine_pos_embed:\n",
    "                feature0, feature1 = feature_add_position(feature0, feature1, attn_splits, self.feature_channels)\n",
    "\n",
    "            # Transformer\n",
    "            feature0, feature1 = self.transformer(feature0, feature1, attn_num_splits=attn_splits)\n",
    "\n",
    "            # correlation and softmax\n",
    "            if corr_radius == -1:  # global matching\n",
    "                flow_pred = global_correlation_softmax(feature0, feature1, self.pred_bidir_flow)[0]\n",
    "            else:  # local matching\n",
    "                flow_pred = local_correlation_softmax(feature0, feature1, corr_radius)[0]\n",
    "\n",
    "            # flow or residual flow\n",
    "            flow = flow + flow_pred if flow is not None else flow_pred\n",
    "\n",
    "            # upsample to the original resolution for supervison\n",
    "            if self.training:  # only need to upsample intermediate flow predictions at training time\n",
    "                flow_bilinear = self.upsample_flow(flow, None, bilinear=True, upsample_factor=upsample_factor)\n",
    "                flow_preds.append(flow_bilinear)\n",
    "\n",
    "            # flow propagation with self-attn\n",
    "            if self.pred_bidir_flow and scale_idx == 0:\n",
    "                feature0 = torch.cat((feature0, feature1), dim=0)  # [2*B, C, H, W] for propagation\n",
    "            flow = self.feature_flow_attn(feature0, flow.detach(),\n",
    "                                          local_window_attn=prop_radius > 0,\n",
    "                                          local_window_radius=prop_radius)\n",
    "            \n",
    "            # bilinear upsampling at training time except the last one\n",
    "            if self.training and scale_idx < self.num_scales - 1:\n",
    "                flow_up = self.upsample_flow(flow, feature0, bilinear=True, upsample_factor=upsample_factor)\n",
    "                flow_preds.append(flow_up)\n",
    "\n",
    "            if scale_idx == self.num_scales - 1:\n",
    "                flow_up = self.upsample_flow(flow, feature0)\n",
    "                flow_preds.append(flow_up)\n",
    "\n",
    "        results_dict.update({'flow_preds': flow_preds})\n",
    "\n",
    "        if not self.training:\n",
    "            results_dict[\"flow_upsampled\"] = results_dict[\"flow_preds\"][0]\n",
    "\n",
    "        return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "222cd703-95c6-479c-bb90-4f58bc82dbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg(cfg_path='../configs/gmflow/models/gmflow_v02.yaml', custom=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2db6476-49a0-46f7-ab2f-6cc825ff8e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfold input: torch.Size([1, 1, 96, 128])\n",
      "\n",
      "\n",
      "unfold input: torch.Size([1, 1, 96, 128])\n",
      "\n",
      "\n",
      "unfold input: torch.Size([1, 1, 48, 64])\n",
      "\n",
      "\n",
      "unfold input: torch.Size([1, 1, 48, 64])\n",
      "\n",
      "\n",
      "unfold input: torch.Size([1, 1, 48, 64])\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goswami.p/miniconda3/envs/ezflow/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811797118/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "model = GMFlowV2(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5c84d6fb-30bc-4222-99de-45489d7aeeae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.466816M params'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(model.backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5456978-01b7-4817-94b9-ac58a17e8842",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1, img2 = torch.randn(1,3,384,512), torch.randn(1,3,384,512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2351ec7-7509-44a8-af8e-e501438b2fee",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "189da088-ba13-4dcd-a112-d647f5dfab40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 3, 368, 496]), torch.Size([1, 3, 368, 496]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img1.shape, img2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dcb97014-20b6-4c98-93b8-f81a7bb006c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.04912M params'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = CNNEncoder(output_dim=128, num_output_scales=1)\n",
    "count_params(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a68438b7-3a73-46fb-9efd-2f68dc874337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = encoder(torch.randn(2, 3, 256, 256))[0]\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8d1428-7481-4e32-8ff5-c033cc471261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/goswami.p/.cache/torch/hub/facebookresearch_dino_main\n"
     ]
    }
   ],
   "source": [
    "from nnflow.models.gmflow import SwinEncoderV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8729e848-6659-49ba-8b11-52dea0a1638f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goswami.p/miniconda3/envs/ezflow/lib/python3.7/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1640811797118/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "encoderv2 = SwinEncoderV2(\n",
    "        embedding_channels=64,\n",
    "        depths=(4, 6),\n",
    "        input_resolution=(384, 512),\n",
    "        number_of_heads=(8, 16),\n",
    "        window_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e09b28-c5d9-47a2-8431-b056e4be44ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoderv2.patch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "82e62925-bbad-46b0-a302-0cea8a71489a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.466816M params'"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(encoderv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "33ed926d-581b-406f-859b-e2b5fd2215d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 64, 96, 128])\n",
      "torch.Size([1, 128, 48, 64])\n"
     ]
    }
   ],
   "source": [
    "feats = encoderv2(torch.randn(1,3, 384, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "a018729e-ba36-4f21-a00b-e2b4b5d301ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 48, 64])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759f1d2-d78c-46b5-b095-6a5bf18cd607",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e7783a-666c-4d35-befc-49e1145cddbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079aa3ff-07b0-4a88-bdbe-cc61d18ece90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Swinv2Config, Swinv2Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6a9fdb-21bd-40aa-b3fc-15e866ef2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "configuration = Swinv2Config()\n",
    "configuration.depths=[4, 6]\n",
    "configuration.embed_dim=64\n",
    "configuration.num_heads=[8,16]\n",
    "configuration.window_size=8\n",
    "# configuration.image_size=[368,496]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4485b8dd-c574-4b36-ac53-d2097ce2e3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Swinv2Config {\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"depths\": [\n",
       "    4,\n",
       "    6\n",
       "  ],\n",
       "  \"drop_path_rate\": 0.1,\n",
       "  \"embed_dim\": 64,\n",
       "  \"encoder_stride\": 32,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"mlp_ratio\": 4.0,\n",
       "  \"model_type\": \"swinv2\",\n",
       "  \"num_channels\": 3,\n",
       "  \"num_heads\": [\n",
       "    8,\n",
       "    16\n",
       "  ],\n",
       "  \"num_layers\": 4,\n",
       "  \"patch_size\": 4,\n",
       "  \"path_norm\": true,\n",
       "  \"pretrained_window_sizes\": [\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0\n",
       "  ],\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.22.2\",\n",
       "  \"use_absolute_embeddings\": false,\n",
       "  \"window_size\": 8\n",
       "}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883a5440-745c-4ab6-85e3-fd25f4c4f5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoderv3 = Swinv2Model(configuration, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97021969-d530-4e57-9260-a1eb93e58eb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.506112M params'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(encoderv3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe79554-942e-45e8-880e-97621bd80252",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = encoderv3(torch.randn(1,3, 368, 496), return_dict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "049fc4d5-403b-4757-9c01-c42a0c328104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, 2)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(feats), len(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3373c035-8c05-4ab0-b018-c9a92a7a2b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 2852, 128]), None)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats[0].shape, feats[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "1a897a74-7622-4e82-9139-513ae6dad48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368, 496)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, C, H, W = img1.shape\n",
    "H, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4ea5fb7-9e87-4214-bca5-5e3d9110eddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46.0, 62.0, 2852)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "368 / 8, 496 /8, 46 * 62"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "07b92953-ef17-4ea0-beec-f8e57b8efd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 2852])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = feats[0].permute(0,2,1)\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7a24db70-ddd2-4a02-83a8-3e1b00a4ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "b,c,_ = feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b109cac8-3b9c-4f35-800d-0bc3d74c0bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 46, 62])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_feats = feats.reshape(b,c, 46, 62)\n",
    "_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "20fd1658-88f9-4463-8046-2a619e523db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d012f97c-df6a-4dc5-8a67-4e51e3c78e2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "EinopsError",
     "evalue": " Error while processing rearrange-reduction pattern \"n (h/8 w/8) c -> n c h w\".\n Input tensor shape: torch.Size([1, 2852, 128]). Additional info: {}.\n Unknown character '/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/miniconda3/envs/ezflow/lib/python3.7/site-packages/einops/einops.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mhashable_axes_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0mrecipe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_prepare_transformation_recipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes_lengths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhashable_axes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrecipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ezflow/lib/python3.7/site-packages/einops/einops.py\u001b[0m in \u001b[0;36m_prepare_transformation_recipe\u001b[0;34m(pattern, operation, axes_lengths)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrght\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'->'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0mleft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParsedExpression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m     \u001b[0mrght\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParsedExpression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrght\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ezflow/lib/python3.7/site-packages/einops/parsing.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, expression)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mEinopsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown character '{}'\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEinopsError\u001b[0m: Unknown character '/'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mEinopsError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9031/1308593376.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrearrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n (h/8 w/8) c -> n c h w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ezflow/lib/python3.7/site-packages/einops/einops.py\u001b[0m in \u001b[0;36mrearrange\u001b[0;34m(tensor, pattern, **axes_lengths)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rearrange can't be applied to an empty list\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack_on_zeroth_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rearrange'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ezflow/lib/python3.7/site-packages/einops/einops.py\u001b[0m in \u001b[0;36mreduce\u001b[0;34m(tensor, pattern, reduction, **axes_lengths)\u001b[0m\n\u001b[1;32m    388\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'\\n Input is list. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'Additional info: {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxes_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mEinopsError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEinopsError\u001b[0m:  Error while processing rearrange-reduction pattern \"n (h/8 w/8) c -> n c h w\".\n Input tensor shape: torch.Size([1, 2852, 128]). Additional info: {}.\n Unknown character '/'"
     ]
    }
   ],
   "source": [
    "feat = rearrange(feats[0], 'n (h w) c -> n c h w')\n",
    "feat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65903fbc-e333-4d5b-b99b-3a4c709e4ee3",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
